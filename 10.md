# Chapter 10

## Key Terms

1. **Hypothesis Testing**: A statistical method that is used in making decisions using data from a scientific study.
2. **Tests of Significance**: These are used to determine whether a particular set of results is statistically significant.
3. **P-Values**: The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.
4. **Type I Error**: The incorrect rejection of a true null hypothesis (a "false positive").
5. **Type II Error**: The failure to reject a false null hypothesis (a "false negative").

## Formulas

1. **P-Value Formula**: P-value = P(T > t) where T is the test statistic and t is the observed value of the test statistic.
2. **Type I Error Rate**: The significance level α.
3. **Type II Error Rate**: β = P(Type II Error) = P(Fail to reject H0 | H0 is false).
4. **Standard Error (SE)**: SE = σ / √n where σ is the standard deviation and n is the sample size.
5. **Confidence Interval**: x̄ ± (t * (s / √n)) where x̄ is the sample mean, t is the t-score for our confidence level, s is the sample standard deviation, and n is the sample size.

## Notes

- Hypothesis testing is a statistical method that is used in making decisions using data. It involves the statement of a null hypothesis, and the selection of a level of significance.
- The p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before the test is performed, a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% and denoted as α.
- If the p-value is equal to or smaller than the significance level (α), it suggests that the observed data is inconsistent with the assumption that the null hypothesis is true, and thus that hypothesis must be rejected and the alternative hypothesis is accepted as true.
- Type I error is the event of rejecting a null hypothesis when it is actually true. This is a false positive. The probability of making a type I error is α, which is the level of significance you set for your hypothesis test.
- Type II error is the event that you accept the null hypothesis when it is actually false. This is a false negative. The probability of making a type II error is β, which depends on the power of the test.

## Approach to Questions

**Hypothesis Testing**:

1. Step 1: State the null hypothesis (H0) and the alternative hypothesis (H1). The null hypothesis is usually a statement of no effect or no difference while the alternative hypothesis is a statement of an effect or difference.
2. Step 2: Choose the level of significance (α). This is the probability of rejecting the null hypothesis when it is true. Common choices are 0.05 (5% chance of Type I error), 0.01 (1% chance), and 0.001 (0.1% chance). The choice depends on the trade-off between the risks of Type I and Type II errors. Lower α reduces the risk of Type I errors but increases the risk of Type II errors.
3. Step 3: Choose the appropriate statistical test. This depends on the nature of your data and your research question. Here are a few examples:
- Use a t-test if you are comparing the means of two groups.
- Use a chi-square test if you are examining the association between two categorical variables.
4. Step 4: Calculate the test statistic. The formula depends on the statistical test you are using. Here are a few examples:
For a t-test: t = (x̄ - μ) / (s / √n) where x̄ is the sample mean, μ is the population mean, s is the sample standard deviation, and n is the sample size.
For a chi-square test: χ² = Σ [ (O - E)² / E ] where O is the observed frequency and E is the expected frequency.
5. Step 5: Determine the critical region using the level of significance. The critical region is the set of all values of the test statistic that cause us to reject the null hypothesis. For a t-test, the critical region is typically given by t > tα where tα is the critical value of t for a given level of significance α.
6. Step 6: Make a decision to either reject or fail to reject the null hypothesis based on the test statistic and the critical region. If the test statistic falls in the critical region, reject the null hypothesis. Otherwise, fail to reject the null hypothesis.

**Tests of Significance**: 
1. Step 1: State the null hypothesis and the alternative hypothesis.
2. Step 2: Choose the level of significance (α) and the statistical test to apply.
3. Step 3: Calculate the test statistic. The formula for the test statistic depends on the statistical test you are using.
4. Step 4: Calculate the p-value. The p-value is the probability of obtaining a test statistic as extreme as the one you calculated, assuming the null hypothesis is true. The formula for the p-value depends on the statistical test you are using.
5. Step 5: If the p-value is less than α, reject the null hypothesis.


